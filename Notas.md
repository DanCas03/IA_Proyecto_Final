# Clasificador Sem√°ntico de Textos Cl√°sicos

Sistema de Inteligencia Artificial para clasificar fragmentos de textos cl√°sicos en tres categor√≠as tem√°ticas:
- **Aret√©**: Excelencia y virtud moral
- **Pol√≠tica y Poder**: Reflexiones sobre gobierno y autoridad
- **Relaci√≥n Dioses-Humanos**: Interacciones entre lo divino y lo mortal

## Requisitos

- Python 3.9+
- MongoDB Atlas (cluster activo)
- 8GB RAM m√≠nimo para entrenamiento

## Instalaci√≥n

1. **Clonar el repositorio:**
```bash
git clone <url-repositorio>
cd proyecto2
```

2. **Crear entorno virtual:**
```bash
python -m venv venv
.\venv\Scripts\Activate.ps1  # Windows PowerShell
# En Linux/Mac: source venv/bin/activate
```

3. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

4. **Configurar MongoDB:**

Copia el archivo `.env.example` como `.env` y completa con tus credenciales:
```bash
copy .env.example .env  # Windows
# cp .env.example .env  # Linux/Mac
```

Edita `.env` con tu informaci√≥n de MongoDB Atlas:
```
MONGO_URI=mongodb+srv://<usuario>:<password>@<cluster>.mongodb.net/?retryWrites=true&w=majority
MONGO_DB_NAME=textos_clasicos
```

> ‚ö†Ô∏è **IMPORTANTE**: Nunca subas el archivo `.env` a Git. Ya est√° incluido en `.gitignore`.

## Uso

### Pipeline Completo (Recomendado)

Ejecutar todo el pipeline de una vez:
```bash
python run_pipeline.py
```

### Ejecuci√≥n por Pasos

1. **ETL (Extracci√≥n de datos):**
```bash
python run_pipeline.py --step etl
```

2. **Preprocesamiento:**
```bash
python run_pipeline.py --step preprocess
```

3. **Entrenamiento:**
```bash
python run_pipeline.py --step train
```

4. **Evaluaci√≥n:**
```bash
python run_pipeline.py --step evaluate
```

### Aplicaci√≥n Web

Una vez entrenado el modelo:
```bash
streamlit run src/app/streamlit_app.py
```

## Estructura del Proyecto

```
proyecto2/
‚îú‚îÄ‚îÄ Dataset/                    # Archivos Excel con datos
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py              # Conexi√≥n MongoDB
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl.py             # Pipeline de extracci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py   # Preprocesamiento y balanceo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py           # Entrenamiento del modelo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py        # Evaluaci√≥n y m√©tricas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference.py       # Inferencia en tiempo real
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îî‚îÄ‚îÄ streamlit_app.py   # Interfaz de usuario
‚îú‚îÄ‚îÄ scripts/                    # Scripts de ejecuci√≥n paso a paso
‚îÇ   ‚îú‚îÄ‚îÄ 01_test_connection.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_run_etl.py
‚îÇ   ‚îú‚îÄ‚îÄ 03_preprocess.py
‚îÇ   ‚îú‚îÄ‚îÄ 04_train.py
‚îÇ   ‚îú‚îÄ‚îÄ 05_evaluate.py
‚îÇ   ‚îî‚îÄ‚îÄ 06_run_app.py
‚îú‚îÄ‚îÄ models/                     # Modelos entrenados (ignorado en git)
‚îú‚îÄ‚îÄ reports/                    # Reportes de evaluaci√≥n
‚îú‚îÄ‚îÄ notebooks/                  # Notebooks de exploraci√≥n
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run_pipeline.py             # Script principal (ejecuta todo)
‚îî‚îÄ‚îÄ README.md
```

## Tecnolog√≠as Utilizadas

- **Python 3.9+**
- **MongoDB Atlas**: Base de datos NoSQL
- **Hugging Face Transformers**: Modelos de lenguaje
- **BETO**: BERT pre-entrenado en espa√±ol
- **Streamlit**: Interfaz de usuario
- **scikit-learn**: M√©tricas y evaluaci√≥n

## Caracter√≠sticas del Sistema ETL

### üîç Detecci√≥n Inteligente de Datos

El sistema ETL implementa estrategias avanzadas para manejar la heterogeneidad de los datos:

1. **Detecci√≥n Difusa de Categor√≠as**: 
   - Maneja variaciones: "Aret√©", "arete", "etiqueta aret√©"
   - Soporta abreviaciones: "H. y D." ‚Üí "Humanos y Dioses"
   - Normaliza autom√°ticamente acentos y espacios

2. **Detecci√≥n de M√∫ltiples Formatos**:
   - **Formato Normal**: Una categor√≠a por hoja (mayor√≠a de archivos)
   - **Formato Multi-Tabla**: M√∫ltiples categor√≠as en paralelo (archivo 4.xlsx)

3. **B√∫squeda Inteligente de Encabezados**:
   - Escanea hasta 20 filas para encontrar headers
   - Maneja tablas que no empiezan en A1
   - Detecta columnas: "Canto", "Versos", "Cita" con fuzzy matching

### üìä Resultados de Extracci√≥n

- **123 documentos** extra√≠dos de 7 archivos Excel
- Distribuci√≥n balanceada:
  - Aret√©: 42 registros (34%)
  - Pol√≠tica y Poder: 38 registros (31%)
  - Relaci√≥n Dioses-Humanos: 43 registros (35%)

## M√©tricas

El modelo debe alcanzar un **F1-Score ‚â• 0.80** en el conjunto de prueba para considerarse viable.

## Publicaci√≥n en Git

### Antes de hacer commit:

1. **Verifica que `.env` NO est√© incluido:**
```bash
git status
# .env NO debe aparecer en la lista
```

2. **Archivos que S√ç debes subir:**
   - Todo el c√≥digo fuente (`src/`, `run_pipeline.py`)
   - `requirements.txt`
   - `README.md`, `.gitignore`
   - `.env.example` (plantilla sin credenciales)
   - Dataset (si es de uso p√∫blico)
   - Estructura de carpetas (`models/.gitkeep`, etc.)

3. **Archivos que NO debes subir (ya ignorados):**
   - `.env` (contiene credenciales)
   - `venv/` (entorno virtual)
   - `models/` (modelos entrenados son muy pesados)
   - `reports/` (pueden regenerarse)
   - `__pycache__/` (archivos compilados de Python)


## Notas Importantes

- **Modelos entrenados**: Los modelos son muy grandes (>500MB) y no se suben a Git. Cada usuario debe entrenar su propio modelo localmente.
- **Dataset**: Si el dataset es privado o tiene derechos de autor, considera no subirlo y documentar d√≥nde obtenerlo.
- **Credenciales**: NUNCA subas archivos `.env` con credenciales reales.

## Autor

Desarrollado como proyecto acad√©mico de IA.
