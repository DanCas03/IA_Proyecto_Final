# Clasificador Sem√°ntico de Textos Cl√°sicos

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MongoDB](https://img.shields.io/badge/MongoDB-Atlas-green.svg)](https://www.mongodb.com/cloud/atlas)
[![Transformers](https://img.shields.io/badge/ü§ó-Transformers-yellow.svg)](https://huggingface.co/transformers/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red.svg)](https://streamlit.io/)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TU_USUARIO/proyecto2/blob/main/colab_setup.ipynb)

Sistema de Inteligencia Artificial para clasificar fragmentos de textos cl√°sicos en tres categor√≠as tem√°ticas:
- **Aret√©**: Excelencia y virtud moral.
- **Pol√≠tica y Poder**: Reflexiones sobre gobierno y autoridad.
- **Relaci√≥n Dioses-Humanos**: Interacciones entre lo divino y lo mortal.

> üìñ **Quick Start:** [Local](README.md#-fase-1-configuraci√≥n-inicial) | [Google Colab](QUICKSTART_COLAB.md) | [Gu√≠a Completa Colab](GUIA_COLAB.md)

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Python 3.9+**
- **MongoDB Atlas**: Base de datos NoSQL para persistencia.
- **Hugging Face Transformers**: Modelos de lenguaje avanzados.
- **BETO**: BERT pre-entrenado en espa√±ol para el an√°lisis sem√°ntico.
- **Streamlit**: Interfaz de usuario moderna y accesible.
- **scikit-learn**: Herramientas de evaluaci√≥n y m√©tricas.

## üìÅ Estructura del Proyecto

```
proyecto2/
‚îú‚îÄ‚îÄ Dataset/                    # Archivos Excel con datos originales
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py              # Conexi√≥n y gesti√≥n de MongoDB
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl.py             # Pipeline de extracci√≥n (ETL)
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py   # Limpieza y balanceo de datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py           # Entrenamiento del modelo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py        # Evaluaci√≥n y reportes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference.py       # M√≥dulo para predicciones
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îî‚îÄ‚îÄ streamlit_app.py   # Aplicaci√≥n web Streamlit
‚îú‚îÄ‚îÄ scripts/                    # Scripts de ejecuci√≥n paso a paso
‚îÇ   ‚îú‚îÄ‚îÄ 01_test_connection.py
‚îÇ   ‚îú‚îÄ‚îÄ 02_run_etl.py
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ models/                     # Modelos entrenados (ignorado en git)
‚îú‚îÄ‚îÄ reports/                    # Gr√°ficos y reportes de m√©tricas
‚îî‚îÄ‚îÄ run_pipeline.py             # Script maestro del pipeline
```

---

# üìñ Gu√≠a de Ejecuci√≥n Paso a Paso

Esta gu√≠a te llevar√° a trav√©s de todo el proceso, desde la configuraci√≥n inicial hasta tener la aplicaci√≥n funcionando.

## üìã Requisitos Previos

- ‚úÖ Python 3.9+ instalado
- ‚úÖ MongoDB Atlas cluster activo
- ‚úÖ 8GB RAM m√≠nimo
- ‚úÖ Espacio en disco: ~2GB (modelos + datos)

> üí° **¬øNo tienes GPU?** Usa [Google Colab](QUICKSTART_COLAB.md) para entrenar **10-20x m√°s r√°pido** de forma gratuita.

## üöÄ Fase 1: Configuraci√≥n Inicial

### 1. Preparar el Entorno

```powershell
# Navegar al directorio del proyecto

# Activar entorno virtual (ya creado)
.\venv\Scripts\Activate.ps1

# Verificar instalaci√≥n de dependencias
python -c "import torch; import transformers; print('Dependencias OK')"
```

### 2. Configurar MongoDB

1. Crea un archivo `.env` en la ra√≠z del proyecto
2. Agrega tu URI de conexi√≥n:

```
MONGO_URI=mongodb+srv://TU_USUARIO:TU_PASSWORD@TU_CLUSTER.mongodb.net/?retryWrites=true&w=majority
MONGO_DB_NAME=textos_clasicos
```

3. **Importante**: Reemplaza `TU_USUARIO`, `TU_PASSWORD` y `TU_CLUSTER` con tus credenciales reales

### 3. Verificar Conexi√≥n

```powershell
python scripts/01_test_connection.py
```

**Resultado esperado:**
```
‚úì Conexi√≥n exitosa a MongoDB Atlas
‚úì Base de datos: textos_clasicos
‚úì Permisos de lectura/escritura: OK
‚úÖ TODAS LAS PRUEBAS PASARON
```

---

## üìä Fase 2: Migraci√≥n de Datos (ETL)

### Ejecutar ETL

```powershell
python scripts/02_run_etl.py
```

**Tiempo estimado:** 1-2 minutos

**Resultado esperado:**
- 123 documentos insertados
- Distribuci√≥n por categor√≠a:
  - Aret√©: 42
  - Pol√≠tica y Poder: 38
  - Dioses-Humanos: 43

**En caso de problemas:**
```powershell
# Ejecutar con debug para ver detalles
python scripts/02_run_etl.py --debug
```

---

## üîÑ Fase 3: Preprocesamiento

### Ejecutar Preprocesamiento y Balanceo

```powershell
python scripts/03_preprocess.py
```

**Tiempo estimado:** 30 segundos

**Qu√© hace:**
1. Limpia los textos (elimina caracteres especiales, normaliza espacios)
2. Balancea las clases (undersampling por defecto)
3. Divide en train (70%), validation (15%), test (15%)
4. Guarda los conjuntos en MongoDB

**Resultado esperado:**
```
‚úì Documentos balanceados: ~105
‚úì Train: ~73
‚úì Val: ~16
‚úì Test: ~16
```

---

## ü§ñ Fase 4: Entrenamiento del Modelo

### Entrenar BETO

```powershell
python scripts/04_train.py
```

**‚è±Ô∏è Tiempo estimado:**
- **CPU**: 1-3 horas
- **GPU**: 15-30 minutos

**Qu√© hace:**
1. Carga BETO (bert-base-spanish-wwm-cased)
2. Fine-tuning con tus datos
3. Guarda el mejor modelo basado en F1-Score
4. Early stopping despu√©s de 2 √©pocas sin mejora

**Durante el entrenamiento ver√°s:**
```
Epoch 1/5: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] loss: 0.85 | accuracy: 0.72
Epoch 2/5: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] loss: 0.42 | accuracy: 0.88
...
```

**Resultado esperado:**
```
‚úÖ ENTRENAMIENTO COMPLETADO
   ‚Ä¢ F1 Macro (val): 0.82 ‚úì
   ‚Ä¢ Modelo guardado en: models/clasificador_textos/final
```

### Si el F1 es < 0.80:

1. **Aumenta √©pocas**: Edita `src/config.py` ‚Üí `num_epochs: 10`
2. **Prueba oversampling**: En `preprocessing.py` cambia a `oversample`
3. **Ajusta learning rate**: Prueba `1e-5` o `3e-5`

---

## üìà Fase 5: Evaluaci√≥n

### Generar Reportes

```powershell
python scripts/05_evaluate.py
```

**Tiempo estimado:** 2-3 minutos

**Qu√© genera:**
- `reports/confusion_matrix.png` - Matriz de confusi√≥n visual
- `reports/metrics_by_class.png` - M√©tricas por categor√≠a
- `reports/evaluation_report.json` - Reporte completo en JSON

**Resultado esperado:**
```
üìä RESULTADOS DE EVALUACI√ìN
   ‚Ä¢ Accuracy: 0.85
   ‚Ä¢ F1-Score (macro): 0.82
   ‚úÖ CRITERIO CUMPLIDO: ‚â• 0.80
```

---

## üé® Fase 6: Aplicaci√≥n Web

### Lanzar Streamlit

```powershell
streamlit run src/app/streamlit_app.py
```

**Se abrir√° autom√°ticamente en:** http://localhost:8501

### Uso de la Aplicaci√≥n:

1. **Ingresa un texto** en el √°rea de texto
2. **Haz clic en "Analizar Texto"**
3. **Observa**:
   - Categor√≠a predicha
   - Nivel de confianza (%)
   - Distribuci√≥n de probabilidades

**Ejemplo de texto para probar:**
```
"La virtud es el camino hacia la excelencia del alma."
‚Üí Deber√≠a clasificarse como "Aret√©"
```

---

## üêõ Soluci√≥n de Problemas

### Problema: Error de conexi√≥n a MongoDB

**Soluci√≥n:**
1. Verifica que tu cluster est√© activo en MongoDB Atlas
2. Revisa que el `.env` tenga la URI correcta
3. Verifica que tu IP est√© en la whitelist de Atlas

### Problema: "Module not found"

**Soluci√≥n:**
```powershell
pip install -r requirements.txt --upgrade
```

### Problema: Memoria insuficiente durante entrenamiento

**Soluci√≥n:**
1. Reduce batch_size en `src/config.py`:
   ```python
   "batch_size_cpu": 2,  # Reducir de 4 a 2
   ```
2. Considera usar Google Colab con GPU gratuita

### Problema: Emojis no se muestran en terminal

**No es un error** - Los scripts ya manejan esto autom√°ticamente. Ver√°s caracteres extra√±os pero el programa funciona correctamente.

---

## ‚úÖ Verificaci√≥n Final

Revisa que todo funcione:

```powershell
# ‚úì Datos en MongoDB
python -c "from src.data.db import get_collection; print(f'Datos: {get_collection(\"raw_texts\").count_documents({})}')"

# ‚úì Modelo entrenado existe
python -c "from pathlib import Path; print('Modelo OK' if Path('models/clasificador_textos/final').exists() else 'Sin modelo')"

# ‚úì Reportes generados
python -c "from pathlib import Path; print('Reportes OK' if Path('reports/confusion_matrix.png').exists() else 'Sin reportes')"
```

---

## üìö Pr√≥ximos Pasos

1. **Experimentar** con diferentes textos en la app
2. **Revisar** las m√©tricas en `reports/`
3. **Ajustar** hiperpar√°metros si es necesario
4. **Documentar** tus resultados
5. **Publicar** en GitHub (sin `.env` ni modelos)

---

## üîç Caracter√≠sticas del Sistema ETL

El sistema ETL implementa estrategias avanzadas para manejar la heterogeneidad de los archivos suministrados:

1.  **Detecci√≥n Difusa de Categor√≠as**: Maneja variaciones en los nombres de las hojas (ej: "etiqueta aret√©", "Aret√©") y abreviaciones ("H. y D.").
2.  **Detecci√≥n de M√∫ltiples Formatos**: Soporta tanto hojas individuales por categor√≠a como hojas con m√∫ltiples tablas en paralelo (formato multi-columna).
3.  **B√∫squeda Inteligente de Encabezados**: Escanea autom√°ticamente las filas para localizar los encabezados de las tablas (Canto, Versos, Cita), sin importar si la tabla empieza en la celda A1.

---

## üö© Publicaci√≥n en Git y Notas Importantes

### Antes de hacer commit:
1.  **Verifica `.env`**: Aseg√∫rate de que tus credenciales de MongoDB NO se suban. Ya est√° en el `.gitignore`.
2.  **Modelos Pesados**: Los modelos entrenados (>500MB) est√°n ignorados; cada usuario debe entrenar el suyo localmente.
3.  **Dataset**: El dataset original est√° incluido en el repositorio para facilitar la primera ejecuci√≥n, pero considera los derechos de autor antes de hacerlo p√∫blico.

### Archivos a subir:
- C√≥digo fuente (`src/`, `run_pipeline.py`, `scripts/`).
- `requirements.txt`, `README.md`, `.gitignore`, `.env.example`.
- Carpetas de estructura con `.gitkeep` (`models/`, `reports/`).

---

## üìû Ayuda Adicional

Si encuentras problemas:
1. Revisa esta gu√≠a
2. Ejecuta scripts con `--debug` cuando est√© disponible
3. Revisa los logs de error completos
4. Consulta la documentaci√≥n de las librer√≠as (Hugging Face, PyTorch)

¬°√âxito con tu proyecto! üéâ

---

## üöÄ Opci√≥n Alternativa: Google Colab

Si no tienes una GPU potente, puedes entrenar el modelo en **Google Colab** de forma gratuita.

### Ventajas:
- ‚úÖ **GPU Tesla T4 gratuita**
- ‚úÖ **10-20x m√°s r√°pido** que CPU
- ‚úÖ **Sin instalaci√≥n local**

### C√≥mo usar:

1. Sube el c√≥digo a GitHub
2. Abre `colab_setup.ipynb` en [Google Colab](https://colab.research.google.com/)
3. Sigue las instrucciones del notebook

üìñ **Gu√≠a completa:** Ver [GUIA_COLAB.md](GUIA_COLAB.md)

---

# Autor
Daniel Castellanos  
Desarrollado como proyecto acad√©mico de IA.
