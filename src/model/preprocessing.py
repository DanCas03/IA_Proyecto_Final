"""
Preprocesamiento y balanceo de datos para entrenamiento.
Lee datos de MongoDB y genera conjuntos balanceados.
"""

import re
from typing import Dict, List, Tuple
from collections import Counter
import random
from sklearn.model_selection import train_test_split
from src.data.db import get_collection, clear_collection

# Mapeo de categor√≠as a IDs num√©ricos
LABEL_MAP = {
    "arete": 0,
    "politica_poder": 1,
    "dioses_hombres": 2
}

LABEL_NAMES = {v: k for k, v in LABEL_MAP.items()}


def clean_text(text: str) -> str:
    """
    Limpia y normaliza el texto de forma exhaustiva para mejorar el entrenamiento.
    
    Args:
        text: Texto crudo
    
    Returns:
        Texto limpio y normalizado
    """
    if not isinstance(text, str):
        return ""
    
    # Convertir a string si es necesario y asegurar codificaci√≥n correcta
    text = str(text).encode('utf-8', errors='ignore').decode('utf-8')
    
    # Eliminar caracteres de control y caracteres no imprimibles
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # Eliminar URLs (si existen)
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # Eliminar emails (si existen)
    text = re.sub(r'\S+@\S+\.\S+', '', text)
    
    # Normalizar comillas: convertir comillas tipogr√°ficas a comillas simples/dobles est√°ndar
    text = re.sub(r'[""¬´¬ª‚Äû‚Äö]', '"', text)  # Comillas dobles tipogr√°ficas
    text = re.sub(r'[''¬¥`]', "'", text)    # Comillas simples tipogr√°ficas
    
    # Normalizar guiones: convertir guiones largos a guiones cortos
    text = re.sub(r'[‚Äî‚Äì]', '-', text)
    
    # Normalizar espacios: eliminar espacios m√∫ltiples, tabs, saltos de l√≠nea m√∫ltiples
    text = re.sub(r'[ \t]+', ' ', text)      # M√∫ltiples espacios/tabs a uno
    text = re.sub(r'\n\s*\n+', '\n', text)   # M√∫ltiples saltos de l√≠nea a uno
    text = re.sub(r'[ \t]*\n[ \t]*', ' ', text)  # Saltos de l√≠nea a espacios
    
    # Eliminar espacios al inicio y final de puntuaci√≥n
    text = re.sub(r'\s+([,.!?;:])', r'\1', text)  # Espacios antes de puntuaci√≥n
    text = re.sub(r'([,.!?;:])\s+', r'\1 ', text)  # Espacios despu√©s de puntuaci√≥n (normalizar)
    
    # Eliminar puntos m√∫ltiples (pero mantener puntos suspensivos como uno solo)
    text = re.sub(r'\.{3,}', '...', text)
    
    # Eliminar espacios m√∫ltiples finales
    text = re.sub(r'\s+', ' ', text)
    
    # Eliminar caracteres no ASCII problem√°ticos pero mantener acentos y caracteres especiales del espa√±ol
    # Permitir letras, n√∫meros, puntuaci√≥n b√°sica, acentos, √±, caracteres latinos
    # text = re.sub(r'[^\w\s.,!?;:()\-"\'√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú]', '', text)  # Comentado: puede ser muy agresivo
    
    # Eliminar caracteres Unicode problem√°ticos pero mantener el espa√±ol y lat√≠n
    # Mantener: letras (incluyendo acentos), n√∫meros, espacios, puntuaci√≥n com√∫n
    # Permitir caracteres latinos b√°sicos y acentos comunes
    text = re.sub(r'[^\w\s.,!?;:()\[\]{}"\'\-√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë√º√ú√†√®√¨√≤√π√Ä√à√å√í√ô√¢√™√Æ√¥√ª√Ç√ä√é√î√õ√ß√á]', '', text)
    
    # Normalizar espacios nuevamente despu√©s de eliminar caracteres
    text = re.sub(r'\s+', ' ', text)
    
    # Eliminar espacios al inicio y final
    text = text.strip()
    
    # Eliminar textos que son solo puntuaci√≥n o espacios
    if not text or text.strip() == '' or re.match(r'^[^\w]+$', text):
        return ""
    
    return text


def get_category_distribution() -> Dict[str, int]:
    """Obtiene la distribuci√≥n de categor√≠as en raw_texts."""
    collection = get_collection("raw_texts")
    pipeline = [
        {"$group": {"_id": "$categoria", "count": {"$sum": 1}}}
    ]
    result = list(collection.aggregate(pipeline))
    return {item["_id"]: item["count"] for item in result}


def balance_by_undersampling(documents: List[Dict]) -> List[Dict]:
    """
    Balancea las clases mediante subsampling de la clase mayoritaria.
    
    Args:
        documents: Lista de documentos con campo 'categoria'
    
    Returns:
        Lista balanceada de documentos
    """
    # Agrupar por categor√≠a
    by_category = {}
    for doc in documents:
        cat = doc["categoria"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(doc)
    
    # Encontrar el m√≠nimo
    min_count = min(len(docs) for docs in by_category.values())
    
    # Subsamplear cada categor√≠a
    balanced = []
    for cat, docs in by_category.items():
        sampled = random.sample(docs, min_count)
        balanced.extend(sampled)
    
    # Mezclar
    random.shuffle(balanced)
    
    return balanced


def balance_by_oversampling(documents: List[Dict]) -> List[Dict]:
    """
    Balancea las clases mediante oversampling de las clases minoritarias.
    
    Args:
        documents: Lista de documentos con campo 'categoria'
    
    Returns:
        Lista balanceada de documentos
    """
    # Agrupar por categor√≠a
    by_category = {}
    for doc in documents:
        cat = doc["categoria"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(doc)
    
    # Encontrar el m√°ximo
    max_count = max(len(docs) for docs in by_category.values())
    
    # Oversamplear cada categor√≠a
    balanced = []
    for cat, docs in by_category.items():
        if len(docs) < max_count:
            # Duplicar documentos hasta alcanzar max_count
            oversampled = docs.copy()
            while len(oversampled) < max_count:
                oversampled.extend(random.sample(docs, min(len(docs), max_count - len(oversampled))))
            balanced.extend(oversampled[:max_count])
        else:
            balanced.extend(docs)
    
    # Mezclar
    random.shuffle(balanced)
    
    return balanced


def preprocess_and_balance(
    balance_strategy: str = "undersample",
    test_size: float = 0.15,
    val_size: float = 0.15,
    random_state: int = 42
) -> Dict:
    """
    Pipeline completo de preprocesamiento y balanceo.
    
    Args:
        balance_strategy: "undersample" o "oversample"
        test_size: Proporci√≥n para conjunto de prueba
        val_size: Proporci√≥n para conjunto de validaci√≥n
        random_state: Semilla para reproducibilidad
    
    Returns:
        Estad√≠sticas del proceso
    """
    random.seed(random_state)
    
    print("=" * 60)
    print("üîÑ Iniciando Preprocesamiento y Balanceo")
    print("=" * 60)
    
    # Obtener datos crudos
    raw_collection = get_collection("raw_texts")
    documents = list(raw_collection.find({}))
    
    print(f"\nüì• Documentos cargados: {len(documents)}")
    
    # Distribuci√≥n inicial
    initial_dist = Counter(doc["categoria"] for doc in documents)
    print(f"\nüìä Distribuci√≥n inicial:")
    for cat, count in initial_dist.items():
        print(f"   ‚Ä¢ {cat}: {count}")
    
    # Limpiar textos
    print("\nüßπ Limpiando textos...")
    cleaned_count = 0
    removed_count = 0
    
    for doc in documents:
        original_text = doc.get("texto", "")
        cleaned_text = clean_text(original_text)
        doc["texto_limpio"] = cleaned_text
        
        if cleaned_text:
            cleaned_count += 1
        else:
            removed_count += 1
    
    # Filtrar documentos sin texto v√°lido (m√°s estricto: m√≠nimo 20 caracteres)
    initial_count = len(documents)
    documents = [d for d in documents if len(d.get("texto_limpio", "")) >= 20]
    removed_short = initial_count - len(documents)
    
    print(f"   Textos limpiados: {cleaned_count}")
    print(f"   Textos vac√≠os eliminados: {removed_count}")
    print(f"   Textos muy cortos eliminados (<20 chars): {removed_short}")
    print(f"   Documentos v√°lidos despu√©s de limpieza: {len(documents)}")
    
    # Estad√≠sticas adicionales de limpieza
    if documents:
        avg_length = sum(len(d.get("texto_limpio", "")) for d in documents) / len(documents)
        min_length = min(len(d.get("texto_limpio", "")) for d in documents)
        max_length = max(len(d.get("texto_limpio", "")) for d in documents)
        print(f"   Longitud promedio: {avg_length:.1f} caracteres")
        print(f"   Longitud m√≠nima: {min_length}, m√°xima: {max_length}")
    
    # Balancear
    print(f"\n‚öñÔ∏è Aplicando estrategia de balanceo: {balance_strategy}")
    if balance_strategy == "undersample":
        balanced_docs = balance_by_undersampling(documents)
        justification = (
            "Se eligi√≥ undersampling para evitar overfitting en clases minoritarias "
            "y mantener la diversidad natural de los datos."
        )
    else:
        balanced_docs = balance_by_oversampling(documents)
        justification = (
            "Se eligi√≥ oversampling para maximizar la cantidad de datos de entrenamiento "
            "sin perder informaci√≥n de ninguna clase."
        )
    
    balanced_dist = Counter(doc["categoria"] for doc in balanced_docs)
    print(f"\nüìä Distribuci√≥n despu√©s de balanceo:")
    for cat, count in balanced_dist.items():
        print(f"   ‚Ä¢ {cat}: {count}")
    print(f"\nüìù Justificaci√≥n: {justification}")
    
    # Preparar datos para split
    texts = [doc["texto_limpio"] for doc in balanced_docs]
    labels = [LABEL_MAP[doc["categoria"]] for doc in balanced_docs]
    
    # Split estratificado
    print(f"\n‚úÇÔ∏è Dividiendo datos (train={1-test_size-val_size:.0%}, val={val_size:.0%}, test={test_size:.0%})...")
    
    # Primero separar test
    X_temp, X_test, y_temp, y_test = train_test_split(
        texts, labels, 
        test_size=test_size, 
        stratify=labels, 
        random_state=random_state
    )
    
    # Luego separar validation del resto
    val_ratio = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp,
        test_size=val_ratio,
        stratify=y_temp,
        random_state=random_state
    )
    
    print(f"   ‚Ä¢ Train: {len(X_train)} samples")
    print(f"   ‚Ä¢ Val: {len(X_val)} samples")
    print(f"   ‚Ä¢ Test: {len(X_test)} samples")
    
    # Guardar en MongoDB
    print("\nüíæ Guardando conjuntos en MongoDB...")
    
    # Limpiar colecciones existentes
    for col_name in ["processed_texts", "train_data", "val_data", "test_data"]:
        clear_collection(col_name)
    
    # Guardar datos procesados completos
    processed_collection = get_collection("processed_texts")
    processed_docs = [
        {"texto": t, "label": l, "categoria": LABEL_NAMES[l]}
        for t, l in zip(texts, labels)
    ]
    processed_collection.insert_many(processed_docs)
    
    # Guardar splits
    def save_split(collection_name: str, X: List[str], y: List[int]):
        collection = get_collection(collection_name)
        docs = [
            {"texto": t, "label": l, "categoria": LABEL_NAMES[l]}
            for t, l in zip(X, y)
        ]
        if docs:
            collection.insert_many(docs)
    
    save_split("train_data", X_train, y_train)
    save_split("val_data", X_val, y_val)
    save_split("test_data", X_test, y_test)
    
    print("   ‚úì Datos guardados exitosamente")
    
    # Estad√≠sticas finales
    stats = {
        "initial_count": len(documents),
        "initial_distribution": dict(initial_dist),
        "balanced_count": len(balanced_docs),
        "balanced_distribution": dict(balanced_dist),
        "balance_strategy": balance_strategy,
        "justification": justification,
        "train_count": len(X_train),
        "val_count": len(X_val),
        "test_count": len(X_test)
    }
    
    print("\n" + "=" * 60)
    print("‚úÖ Preprocesamiento completado")
    print("=" * 60)
    
    return stats


if __name__ == "__main__":
    stats = preprocess_and_balance(balance_strategy="undersample")
